<div align="center">
<img src="./docs/images/icon.jpg" alt="icon" height="160"/>

<h1 align="center">ChatLLM Web</h1>

English / [ç®€ä½“ä¸­æ–‡](./docs/README_CN.md)

ğŸ—£ï¸ Chat with LLM like Vicuna totally in your browser with WebGPU, safely, privately, and with no server. Powered By [web-llm](https://github.com/mlc-ai/web-llm).

[Try it now](https://chat-llm-web.vercel.app)

![cover](./docs/images/cover.png)

</div>

## Features

- ğŸ¤– Everything runs inside the browser with **no server support** and is **accelerated with WebGPU**.

- âš™ï¸ Model runs in a web worker, ensuring that it doesn't block the user interface and providing a seamless experience.

- ğŸš€ Easy to deploy for free with one-click on Vercel in under 1 minute, then you get your own ChatLLM Web.

- ğŸ’¾ Model caching is supported, so you only need to download the model once.

- ğŸ’¬ Multi-conversation chat, with all data stored locally in the browser for privacy.

- ğŸ“ Markdown and streaming response support: math, code highlighting, etc.

- ğŸ¨ responsive and well-designed UI, including dark mode.

- ğŸ’» PWA supported, download it and run totally offline.

## Instructions

- ğŸŒ To use this app, you need a browser that supports WebGPU, such as Chrome 113 or Chrome Canary. Chrome versions â‰¤ 112 are not supported.

- ğŸ’» You will need a GPU with about 6.4GB of memory. If your GPU has less memory, the app will still run, but the response time will be slower.

- ğŸ“¥ The first time you use the app, you will need to download the model. For the Vicuna-7b model that we are currently using, the download size is about 4GB. After the initial download, the model will be loaded from the browser cache for faster usage.

- â„¹ï¸ For more details, please visit [mlc.ai/web-llm](https://mlc.ai/web-llm/)

## Roadmap

- [âœ…] LLM: using web worker to create an LLM instance and generate answers.

- [âœ…] Conversations: Multi-conversation support is available.

- [âœ…] PWA

- [] Settings: 
   - ui: dark/light theme
   - device: 
      - gpu device choose
      - cache usage and manage
   - model: 
      - support multi models: vicuna-7bâœ… RedPajama-INCITE-Chat-3B []
      - params config: temperature, max-length, etc.
      - export & import model

## Deploy to Vercel

1. Click
   [![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FRyan-yang125%2FChatLLM-Web&project-name=chat-llm-web&repository-name=ChatLLM-Web), follow the instructions, and finish in just 1 minute.
2. Enjoy it ğŸ˜Š

## Development

```shell
git clone https://github.com/Ryan-yang125/ChatLLM-Web.git
cd ChatLLM-Web
npm i
npm run dev
```

## Screenshots

![Home](./docs/images/home.png)

![More](./docs/images/mobile.png)

## LICENSE

[MIT](./LICENSE)
